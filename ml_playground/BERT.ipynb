{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cc3e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import hashlib\n",
    "import time\n",
    "import json\n",
    "import multiprocessing\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb060f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "875cfd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUB = {\"wikitext-2\": (\n",
    "    \"https://s3.amazonaws.com/research.metamind.io/wikitext/\" \"wikitext-2-v1.zip\",\n",
    "    \"3c914d17d80b1459be871a5039ac23e752a53cbe\")\n",
    "}\n",
    "\n",
    "CLS_TOKEN = \"<CLS>\"\n",
    "SEP_TOKEN = \"<SEP>\"\n",
    "MASK_TOKEN = \"<MASK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64a1c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, \"wiki.train.tokens\")\n",
    "    with open(file_name, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    paragraphs = [\n",
    "        line.strip().lower().split(\" . \")\n",
    "        for line in lines\n",
    "        if len(line.split(\" . \")) >= 2\n",
    "    ]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def download(name, cache_dir=os.path.join(os.path.expanduser(\"~\"), \"data\")):\n",
    "    if name not in DATA_HUB:\n",
    "        raise KeyError(f\"{name} does not exist in {DATA_HUB}\")\n",
    "\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split(\"/\")[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, \"rb\") as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # Hit cache\n",
    "    print(f\"Downloading {fname} from {url}...\")\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "\n",
    "\n",
    "def download_extract(name, folder=None):\n",
    "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    if ext == \".zip\":\n",
    "        fp = zipfile.ZipFile(fname, \"r\")\n",
    "    elif ext in (\".tar\", \".gz\"):\n",
    "        fp = tarfile.open(fname, \"r\")\n",
    "    else:\n",
    "        assert False, \"Only zip/tar files can be extracted.\"\n",
    "    fp.extractall(base_dir)\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir\n",
    "\n",
    "\n",
    "def tokenize(lines, token=\"word\"):\n",
    "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
    "    if token == \"word\":\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == \"char\":\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print(\"ERROR: unknown token type: \" + token)\n",
    "\n",
    "\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    tokens = [CLS_TOKEN] + tokens_a + [SEP_TOKEN]\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + [SEP_TOKEN]\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ee0e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"\n",
    "    Represents a vocabulary for a text corpus\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "\n",
    "        counter = self._count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.unk, uniq_tokens = 0, [UNK_TOKEN] + reserved_tokens\n",
    "        uniq_tokens += [\n",
    "            token\n",
    "            for token, freq in self.token_freqs\n",
    "            if freq >= min_freq and token not in uniq_tokens\n",
    "        ]\n",
    "        self.idx_to_token, self.token_to_idx = [], {}\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    def _count_corpus(self, tokens):\n",
    "        if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35d127df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        \"\"\"\n",
    "        paragraphs[i] is a list of sentence strings\n",
    "        \"\"\"\n",
    "        paragraphs = [tokenize(paragraph, token=\"word\") for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "        self.vocab = Vocab(\n",
    "            sentences,\n",
    "            min_freq=5,\n",
    "            reserved_tokens=[PAD_TOKEN, MASK_TOKEN, CLS_TOKEN, SEP_TOKEN],\n",
    "        )\n",
    "\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(\n",
    "                self._get_nsp_data_from_paragraph(\n",
    "                    paragraph, paragraphs, self.vocab, max_len\n",
    "                )\n",
    "            )\n",
    "\n",
    "        examples = [\n",
    "            (self._get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next))\n",
    "            for tokens, segments, is_next in examples\n",
    "        ]\n",
    "        # Pad inputs\n",
    "        padded_inputs = self._pad_bert_inputs(examples, max_len, self.vocab)\n",
    "        (\n",
    "            self.all_token_ids,\n",
    "            self.all_segments,\n",
    "            self.valid_lens,\n",
    "            self.all_pred_positions,\n",
    "            self.all_mlm_weights,\n",
    "            self.all_mlm_labels,\n",
    "            self.nsp_labels,\n",
    "        ) = padded_inputs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.all_token_ids[idx],\n",
    "            self.all_segments[idx],\n",
    "            self.valid_lens[idx],\n",
    "            self.all_pred_positions[idx],\n",
    "            self.all_mlm_weights[idx],\n",
    "            self.all_mlm_labels[idx],\n",
    "            self.nsp_labels[idx],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)\n",
    "\n",
    "    # next sentence private methods\n",
    "\n",
    "    def _get_next_sentence(self, sentence, next_sentence, paragraphs):\n",
    "        if random.random() < 0.5:\n",
    "            is_next = True\n",
    "        else:\n",
    "            next_sentence = random.choice(random.choice(paragraphs))\n",
    "            is_next = False\n",
    "        return sentence, next_sentence, is_next\n",
    "\n",
    "    def _get_nsp_data_from_paragraph(self, paragraph, paragraphs, vocab, max_len):\n",
    "        nsp_data_from_paragraph = []\n",
    "        for i in range(len(paragraph) - 1):\n",
    "            tokens_a, tokens_b, is_next = self._get_next_sentence(\n",
    "                paragraph[i], paragraph[i + 1], paragraphs\n",
    "            )\n",
    "            # two tokens, plus 1 <CLS> and 2 <SEP> tokens\n",
    "            if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "                continue\n",
    "            tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "            nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "\n",
    "        return nsp_data_from_paragraph\n",
    "\n",
    "    # masked language training methods\n",
    "    def _replace_mlm_tokens(\n",
    "        self, tokens, candidate_pred_positions, num_mlm_preds, vocab\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Make a new copy of tokens to be used as input of a masked language model, where input\n",
    "        contains either <mask> or random tokens\n",
    "        \"\"\"\n",
    "        mlm_input_tokens = [token for token in tokens]\n",
    "        pred_positions_and_labels = []\n",
    "\n",
    "        random.shuffle(candidate_pred_positions)\n",
    "        for mlm_pred_position in candidate_pred_positions:\n",
    "            if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "                break\n",
    "            masked_token = None\n",
    "            if random.random() < 0.8:\n",
    "                masked_token = MASK_TOKEN\n",
    "            else:\n",
    "                if random.random() < 0.5:  # 10% of the time (0.5 * 0.2)\n",
    "                    masked_token = tokens[mlm_pred_position]\n",
    "                else:\n",
    "                    masked_token = random.randint(0, len(vocab) - 1)\n",
    "            mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "            pred_positions_and_labels.append(\n",
    "                (mlm_pred_position, tokens[mlm_pred_position])\n",
    "            )\n",
    "\n",
    "        return mlm_input_tokens, pred_positions_and_labels\n",
    "\n",
    "    def _get_mlm_data_from_tokens(self, tokens, vocab):\n",
    "        candidate_pred_positions = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in [CLS_TOKEN, SEP_TOKEN]:\n",
    "                continue\n",
    "            candidate_pred_positions.append(i)\n",
    "\n",
    "        num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "        mlm_input_tokens, pred_positions_and_labels = self._replace_mlm_tokens(\n",
    "            tokens, candidate_pred_positions, num_mlm_preds, vocab\n",
    "        )\n",
    "        pred_positions_and_labels = sorted(\n",
    "            pred_positions_and_labels, key=lambda x: x[0]\n",
    "        )\n",
    "        pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "        mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "\n",
    "        return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n",
    "\n",
    "    def _pad_bert_inputs(self, examples, max_len, vocab):\n",
    "        max_num_mlm_preds = round(max_len * 0.15)\n",
    "        all_token_ids = []\n",
    "        all_segments = []\n",
    "        valid_lens = []\n",
    "        all_pred_positions = []\n",
    "        all_mlm_weights = []\n",
    "        all_mlm_labels = []\n",
    "        nsp_labels = []\n",
    "\n",
    "        for (\n",
    "            token_ids,\n",
    "            pred_positions,\n",
    "            mlm_pred_label_ids,\n",
    "            segments,\n",
    "            is_next,\n",
    "        ) in examples:\n",
    "            all_token_ids.append(\n",
    "                torch.tensor(\n",
    "                    token_ids + [vocab[PAD_TOKEN]] * (max_len - len(token_ids)),\n",
    "                    dtype=torch.long,\n",
    "                )\n",
    "            )\n",
    "            all_segments.append(\n",
    "                torch.tensor(\n",
    "                    segments + [0] * (max_len - len(segments)), dtype=torch.long\n",
    "                )\n",
    "            )\n",
    "            valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "            all_pred_positions.append(\n",
    "                torch.tensor(\n",
    "                    pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)),\n",
    "                    dtype=torch.long,\n",
    "                )\n",
    "            )\n",
    "            all_mlm_weights.append(\n",
    "                torch.tensor(\n",
    "                    [1.0] * len(mlm_pred_label_ids)\n",
    "                    + [0.0] * (max_num_mlm_preds - len(pred_positions)),\n",
    "                    dtype=torch.float32,\n",
    "                )\n",
    "            )\n",
    "            all_mlm_labels.append(\n",
    "                torch.tensor(\n",
    "                    mlm_pred_label_ids\n",
    "                    + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)),\n",
    "                    dtype=torch.long,\n",
    "                )\n",
    "            )\n",
    "            nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "        return (\n",
    "            all_token_ids,\n",
    "            all_segments,\n",
    "            valid_lens,\n",
    "            all_pred_positions,\n",
    "            all_mlm_weights,\n",
    "            all_mlm_labels,\n",
    "            nsp_labels,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "89ed9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_wiki(batch_size, max_len):\n",
    "    num_workers = 4\n",
    "    data_dir = download_extract(\"wikitext-2\", \"wikitext-2\")\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    train_set = WikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=0)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3ba21c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3d9dd6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = download_extract(\"wikitext-2\", \"wikitext-2\")\n",
    "paragraphs = _read_wiki(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "26da7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = WikiTextDataset(paragraphs, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4d5e0f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [tokenize(paragraph, token=\"word\") for paragraph in paragraphs]\n",
    "sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "vocab = Vocab(\n",
    "    sentences,\n",
    "    min_freq=5,\n",
    "    reserved_tokens=[PAD_TOKEN, MASK_TOKEN, CLS_TOKEN, SEP_TOKEN],\n",
    ")\n",
    "\n",
    "examples = []\n",
    "for paragraph in paragraphs:\n",
    "    examples.extend(\n",
    "        wiki._get_nsp_data_from_paragraph(\n",
    "            paragraph, paragraphs, vocab, max_len\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fd21a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_2s = [\n",
    "    (wiki._get_mlm_data_from_tokens(tokens, vocab) + (segments, is_next))\n",
    "    for tokens, segments, is_next in examples\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bb1ec82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<CLS>',\n",
       "  'it',\n",
       "  'has',\n",
       "  'a',\n",
       "  'western',\n",
       "  'terminus',\n",
       "  'at',\n",
       "  'wyoming',\n",
       "  'highway',\n",
       "  '151',\n",
       "  '(',\n",
       "  'wyo',\n",
       "  '151',\n",
       "  ')',\n",
       "  'at',\n",
       "  'the',\n",
       "  'wyoming',\n",
       "  '–',\n",
       "  'nebraska',\n",
       "  'state',\n",
       "  'line',\n",
       "  '<SEP>',\n",
       "  'the',\n",
       "  'road',\n",
       "  'travels',\n",
       "  'eastward',\n",
       "  'to',\n",
       "  'n',\n",
       "  '@-@',\n",
       "  '71',\n",
       "  ',',\n",
       "  'where',\n",
       "  'it',\n",
       "  'turns',\n",
       "  'south',\n",
       "  '<SEP>'],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " True)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f3164b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = examples[12][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0547b00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<CLS>',\n",
       " 'it',\n",
       " 'has',\n",
       " 'a',\n",
       " 'western',\n",
       " 'terminus',\n",
       " 'at',\n",
       " 'wyoming',\n",
       " 'highway',\n",
       " '151',\n",
       " '(',\n",
       " 'wyo',\n",
       " '151',\n",
       " ')',\n",
       " 'at',\n",
       " 'the',\n",
       " 'wyoming',\n",
       " '–',\n",
       " 'nebraska',\n",
       " 'state',\n",
       " 'line',\n",
       " '<SEP>',\n",
       " 'the',\n",
       " 'road',\n",
       " 'travels',\n",
       " 'eastward',\n",
       " 'to',\n",
       " 'n',\n",
       " '@-@',\n",
       " '71',\n",
       " ',',\n",
       " 'where',\n",
       " 'it',\n",
       " 'turns',\n",
       " 'south',\n",
       " '<SEP>']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ea4a26a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pred_positions = []\n",
    "for i, token in enumerate(tokens):\n",
    "    if token in [CLS_TOKEN, SEP_TOKEN]:\n",
    "        continue\n",
    "    candidate_pred_positions.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0bdd1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "mlm_input_tokens, pred_positions_and_labels = wiki._replace_mlm_tokens(\n",
    "    tokens, candidate_pred_positions, num_mlm_preds, vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "76de45da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'it'), (2, 'has'), (29, '71'), (30, ','), (34, 'south')]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "47434ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<CLS>',\n",
       "  '1963',\n",
       "  ':',\n",
       "  'stigma',\n",
       "  ':',\n",
       "  'notes',\n",
       "  'on',\n",
       "  'the',\n",
       "  'management',\n",
       "  'of',\n",
       "  '<unk>',\n",
       "  'identity',\n",
       "  '<SEP>',\n",
       "  '<unk>',\n",
       "  '@-@',\n",
       "  'hall',\n",
       "  '<SEP>'],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       " True)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "61eb9e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, 2502, 55, 13777, 2, 928, 16, 5, 1250, 7, 2, 2892, 4, 8, 15, 735, 4],\n",
       " [4, 10, 15],\n",
       " [55, 8, 735],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       " True)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_2s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "63d41af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_positions_and_labels = sorted(\n",
    "    pred_positions_and_labels, key=lambda x: x[0]\n",
    ")\n",
    "pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "mlm_pred_labels = [v[1] for v in pred_positions_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d28b9157",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_inputs = wiki._pad_bert_inputs(example_2s[0:10], max_len, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8e9ece9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sent_inputs = [padded_inputs[i][0] for i in range(len(padded_inputs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d8e7498c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17., 17., 17., 17., 17., 17., 14., 14., 14., 14., 14., 14., 32., 32.,\n",
       "        32., 32., 32., 32., 31., 31., 31., 31., 31., 31., 58., 58., 58., 58.,\n",
       "        58., 58., 61., 61., 61., 61., 61., 61., 64., 64., 64., 64., 64., 64.,\n",
       "        41., 41., 41., 41., 41., 41., 24., 24., 24., 24., 24., 24., 38., 38.,\n",
       "        38., 38., 38., 38.])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(torch.tensor(padded_inputs[2]), repeats=6, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "064c6dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2483,  0.5715, -1.6247, -1.7459,  1.2908,  0.7252, -0.5504,  0.2260,\n",
       "         -0.8742, -1.7012]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
